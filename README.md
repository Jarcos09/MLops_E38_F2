# E38_Fase_2

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

Fase 2 Avance de Proyecto, Gestion del Proyecto de Machine Learning

## Project Organization

```
â”œâ”€â”€ LICENSE                         <- Open-source license if one is chosen
â”œâ”€â”€ Makefile                        <- Makefile with convenience commands like `make data` or `make train`
â”œâ”€â”€ README.md                       <- The top-level README for developers using this project.
â”œâ”€â”€ params.yaml                     <- Centralized configuration file for pipeline parameters.
â”œâ”€â”€ data                
â”‚   â”œâ”€â”€ external                    <- Data from third party sources.
â”‚   â”œâ”€â”€ interim                     <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed                   <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw                         <- The original, immutable data dump.
â”‚               
â”œâ”€â”€ docs                            <- A default mkdocs project; see www.mkdocs.org for details
â”‚               
â”œâ”€â”€ models                          <- Trained and serialized models, model predictions, or model summaries
â”‚               
â”œâ”€â”€ notebooks                       <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                                      the creator's initials, and a short `-` delimited description, e.g.
â”‚                                      `1.0-jqp-initial-data-exploration`.
â”‚               
â”œâ”€â”€ pyproject.toml                  <- Project configuration file with package metadata for 
â”‚                                      MLFlow/DVC and configuration for tools like black
â”‚               
â”œâ”€â”€ references                      <- Data dictionaries, manuals, and all other explanatory materials.
â”‚               
â”œâ”€â”€ reports                         <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures                     <- Generated graphics and figures to be used in reporting
â”‚               
â”œâ”€â”€ requirements.txt                <- The requirements file for reproducing the analysis environment, e.g.
â”‚                                      generated with `pip freeze > requirements.txt`
â”‚               
â”œâ”€â”€ setup.cfg                       <- Configuration file for flake8
â”‚               
â””â”€â”€ src                             <- Source code for the project
    â”œâ”€â”€ __init__.py                 <- Makes `src` a Python module
    â”œâ”€â”€ utils
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ cmd.py                  <- Functions to execute cmd commands
    â”‚   â””â”€â”€ paths.py                <- Paths manager to create and ensure directories
    â”œâ”€â”€ config
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ dvc_setup.py            <- Functions to set dvc repos
    â”‚   â””â”€â”€ config.py               <- Store useful variables and configuration
    â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ clean_dataset.py        <- Script to clean raw data
    â”‚   â”œâ”€â”€ cleaning.py             <- Main cleaning scripts
    â”‚   â”œâ”€â”€ dataset.py              <- Scripts to download or generate data
    â”‚   â”œâ”€â”€ download_dataset.py     <- Scripts to fetch datasets from external sources
    â”‚   â”œâ”€â”€ features.py             <- Code to create features for modeling
    â”‚   â””â”€â”€ preprocess_data.py      <- Preprocessing pipelines for ML
    â””â”€â”€ modeling
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ plots.py                <- Code to create visualizations
        â”œâ”€â”€ predict.py              <- Code to run model inference with trained models
        â”œâ”€â”€ train_model.py          <- Model training logic and MLFlow integration
        â””â”€â”€ train.py                <- Entry point to train models
```

--------

# Fase 2 | Avance de Proyecto
# Equipo 38

En esta actividad se continuarÃ¡ con el desarrollo del proyecto, dando seguimiento a los avances realizados en la Fase 1. Se mantendrÃ¡ la propuesta de valor, el anÃ¡lisis elaborado con el ML Canvas, asÃ­ como los datos, modelos y experimentos previamente desarrollados. El objetivo ahora es estructurar el proyecto de Machine Learning de forma profesional, aplicando buenas prÃ¡cticas como la refactorizaciÃ³n del cÃ³digo, el control de versiones, el seguimiento de experimentos, el registro de mÃ©tricas y modelos, y el aseguramiento de la reproducibilidad.

--------

## ðŸŽ¯ Objetivos

- Continuar con el desarrollo de proyectos de Machine Learning, a partir de los requerimientos, una propuesta de valor y un conjunto de datos preprocesados.
- Estructurar proyectos de Machine Learning de manera organizada (utilizando el template de Cookiecutter)
- Aplicar buenas prÃ¡cticas de codificaciÃ³n en cada etapa del pipeline y realizar RefactorizaciÃ³n del cÃ³digo.
- Registrar mÃ©tricas y aplicar control de versiones  a los experimentos utilizando herramientas de loging y tracking  (MLFlow/DVC)
- Visualizar y comparar resultados (mÃ©tricas) y gestionar el registro de los modelos (Data Registry MLFlow/DVC)

--------

## ðŸ‘¥ Roles del Equipo
| Integrante | MatrÃ­cula | Rol |
|---|---|---|
| Jaime Alejandro MendÃ­vil Altamirano| `A01253316` | SRE / DevOps |
| Christian Erick Mercado Flores | `A00841954` | Software Engineer  |
| Saul Mora Perea | `A01796295` | Data Engineer  |
| Juan Carlos PÃ©rez Nava | `A01795941` | Data Scientist  |
| Mario Javier Soriano Aguilera | `A01384282` | ML Engineer  |

--------

## ðŸ“¦ Instalar paqueterÃ­as
```bash
pip install -r requirements.txt --quiet
```
## ðŸ’¼ Clonar repositorio
```bash
git clone https://github.com/Jarcos09/MLops_E38_F2.git
cd MLops_E38_F2/
```

--------

## ðŸ“š Makefile

Descargar Dataset:
```bash
make data
```

Realizar limpieza del Dataset:
```bash
make clean_data
```

Realizar FE:
```bash
make FE
```

Ejecuta (data â†’ clean_data â†’ FE):
```bash
make prepare
```

Ejecutar localmente servidor de MLFlow:
```bash
make mlflow-server
```

Realizar entrenamiento:
```bash
make train
```

Realizar preducciÃ³n:
```bash
make predict
```

ConfiguraciÃ³n completa de DVC remoto:
```bash
make dvc_setup
```

Ejecutar el pipeline completo de DVC (data â†’ clean â†’ FE â†’ train):
```bash
make dvc_repro
```

Subir los outputs del pipeline al remoto:
```bash
make dvc_push
```

Descargar los datos versionados del remoto:
```bash
make dvc_pull
```

Verificar quÃ© etapas del pipeline estÃ¡n desactualizadas:
```bash
make dvc_status
```

--------

## ðŸ§  MLflow

**MLflow** es una herramienta para gestionar el ciclo de vida de modelos de Machine Learning: rastrea experimentos, guarda mÃ©tricas y versiona modelos.

---

### Iniciar servidor local

Se puede utilizar el comando:
```bash
make dvc_setup
```

TambiÃ©n se puede ejecutar el servidor en modo local con SQLite y carpeta `mlruns`:
```bash
mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlruns \
    --host 0.0.0.0 \
    --port 5000
````

### Interfaz
http://localhost:5000

### IntegraciÃ³n en el Proyecto
* `train_model.py`: Registra mÃ©tricas, parÃ¡metros y modelos (Random Forest, XGBoost).

* `predict_model.py`: Usa modelos registrados para generar predicciones.

* `config/config.py`: Define la URI de tracking (mlflow_tracking_uri).

--------

## ðŸ’¾ DVC

### InicializaciÃ³n de Repositorio DVC

Se puede utilizar el comando:
```bash
make mlflow-server
```

TambiÃ©n, se puede inicializar manualmente de la siguiente manera:
```bash
dvc init
```
### GDRIVE
#### Agregar Repositorio DVC (GDrive)
```bash
dvc remote add -d data "$GDRIVE_REMOTE_URL"
```

#### ConfiguraciÃ³n de DVC (GDrive)
```bash
dvc remote modify data gdrive_client_id "$GDRIVE_CLIENT_ID"
dvc remote modify data gdrive_client_secret "$GDRIVE_CLIENT_SECRET"
```

### AWS
#### Agregar Repositorio DVC (AWS)
```bash
dvc remote add -d data "$AWS_REMOTE_URL"
```

#### ConfiguraciÃ³n de DVC (AWS)
```bash
dvc remote modify team_remote region "$AWS_REGION"
dvc remote modify team_remote profile "$AWS_PROFILE"
```

### Verificar Repositorios DVC Configurados
```bash
dvc remote list
```

### Repositorio DVC (GDrive)
[Carpeta Principal del Proyecto en Google Drive](https://drive.google.com/drive/u/2/folders/1VnjNYOpP2uSaaUtFdRzW45iwZJUbt-5v)

--------